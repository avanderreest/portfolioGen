{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install yfinance\n",
    "!pip3 install pandas==1.5.3\n",
    "!pip3 install numpy==1.26.4\n",
    "!pip3 install pandas_ta\n",
    "!pip3 install pandas_datareader\n",
    "!pip3 install statsmodels\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install PyPortfolioOpt\n",
    "!pip3 install jupyterlab\n",
    "!pip3 install arch\n",
    "!pip3 install pandas_datareader\n",
    "!pip3 install alpaca-py\n",
    "!pip3 install numpy=\n",
    "!pip2 install datetime\n",
    "!pip3 install sklearn\n",
    "!pip3 install PyPortfolioOpt\n",
    "!pip3 install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc2a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee7defea",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Trading Strategy\n",
    "\n",
    "* Download/Load SP500 stocks prices data.\n",
    "* Calculate different features and indicators on each stock.\n",
    "* Aggregate on monthly level and filter top 150 most liquid stocks.\n",
    "* Calculate Monthly Returns for different time-horizons.\n",
    "* Download Fama-French Factors and Calculate Rolling Factor Betas.\n",
    "* For each month fit a K-Means Clustering Algorithm to group similar assets based on their features.\n",
    "* For each month select assets based on the cluster and form a portfolio based on Efficient Frontier max sharpe ratio optimization.\n",
    "* Visualize Portfolio returns and compare to SP500 returns.\n",
    "* Video at: https://youtu.be/9Y3yaoi9rUQ?si=JnKro_HeAoDGfiht\n",
    "* Source at: https://github.com/Luchkata/Algorithmic_Trading_Machine_Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf4331",
   "metadata": {},
   "source": [
    "# All Packages Needed:\n",
    "* pandas, numpy, matplotlib, statsmodels, pandas_datareader, datetime, yfinance, sklearn, PyPortfolioOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f19f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from datetime import datetime, timedelta\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import pandas_ta\n",
    "import requests\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a1c48",
   "metadata": {},
   "source": [
    "### Download tickers from Nasdaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'PKAFE6FQ1ZTHK0KNPDBU'\n",
    "API_SECRET = 'x6DmwTMWv4Bfbdu9TyCxpV0hNTzIqGmSEtrazsrR'\n",
    "\n",
    "from alpaca.data import StockHistoricalDataClient\n",
    "from alpaca.trading.client import TradingClient\n",
    "from alpaca.trading.requests import GetAssetsRequest\n",
    "from alpaca.trading.enums import AssetClass, AssetStatus\n",
    "from enum import Enum\n",
    "\n",
    "class Market(Enum):\n",
    "    \"\"\"Enum for different market exchanges\"\"\"\n",
    "    NASDAQ = \"NASDAQ\"\n",
    "    NYSE = \"NYSE\"\n",
    "    AMEX = \"AMEX\"\n",
    "    ARCA = \"ARCA\"\n",
    "    BATS = \"BATS\"\n",
    "    IEX = \"IEX\"\n",
    "    ALL = \"ALL\"\n",
    "\n",
    "def get_tradable_tickers(api_key, api_secret, market=Market.ALL):\n",
    "    \"\"\"\n",
    "    Retrieves a list of tradable stock tickers from Alpaca Markets\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Your Alpaca API key\n",
    "        api_secret (str): Your Alpaca API secret\n",
    "        market (Market): The specific market to filter for (default: ALL)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tradable stock symbols\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the trading client\n",
    "        trading_client = TradingClient(api_key, api_secret)\n",
    "        \n",
    "        # Set up the request parameters\n",
    "        search_params = GetAssetsRequest(\n",
    "            asset_class=AssetClass.US_EQUITY,\n",
    "            status=AssetStatus.ACTIVE\n",
    "        )\n",
    "        \n",
    "        # Get all assets\n",
    "        assets = trading_client.get_all_assets(search_params)\n",
    "        \n",
    "        # Filter for tradable assets and specified market\n",
    "        tradable_tickers = []\n",
    "        for asset in assets:\n",
    "            if not asset.tradable:\n",
    "                continue\n",
    "                \n",
    "            if market == Market.ALL:\n",
    "                tradable_tickers.append({\n",
    "                    'symbol': asset.symbol,\n",
    "                    'exchange': asset.exchange\n",
    "                })\n",
    "            elif asset.exchange == market.value:\n",
    "                tradable_tickers.append({\n",
    "                    'symbol': asset.symbol,\n",
    "                    'exchange': asset.exchange\n",
    "                })\n",
    "        \n",
    "        # Sort by symbol\n",
    "        tradable_tickers = sorted(tradable_tickers, key=lambda x: x['symbol'])\n",
    "        \n",
    "        return tradable_tickers\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tickers: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def print_market_summary(tickers):\n",
    "    \"\"\"\n",
    "    Prints a summary of tickers by exchange\n",
    "    \n",
    "    Args:\n",
    "        tickers (list): List of ticker dictionaries\n",
    "    \"\"\"\n",
    "    exchange_count = {}\n",
    "    for ticker in tickers:\n",
    "        exchange = ticker['exchange']\n",
    "        exchange_count[exchange] = exchange_count.get(exchange, 0) + 1\n",
    "    \n",
    "    print(\"\\nMarket Summary:\")\n",
    "    for exchange, count in sorted(exchange_count.items()):\n",
    "        print(f\"{exchange}: {count} tickers\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "    \n",
    "# Get all NASDAQ tickers\n",
    "nasdaq_tickers = get_tradable_tickers(API_KEY, API_SECRET, Market.NASDAQ)\n",
    "# print(f\"\\nFound {len(nasdaq_tickers)} NASDAQ tickers\")\n",
    "# print(\"First 5 NASDAQ tickers:\")\n",
    "# for ticker in nasdaq_tickers[:5]:\n",
    "#     print(f\"Symbol: {ticker['symbol']}, Exchange: {ticker['exchange']}\")\n",
    "    \n",
    "# create a list symbols_list with all the nasdaq tickers\n",
    "symbols_list = []\n",
    "for ticker in nasdaq_tickers:\n",
    "    symbols_list.append(ticker['symbol'])\n",
    "    \n",
    "symbols_list    \n",
    "\n",
    "# Get all tickers across exchanges\n",
    "# all_tickers = get_tradable_tickers(API_KEY, API_SECRET, Market.ALL)\n",
    "# print(f\"\\nFound {len(all_tickers)} total tickers\")\n",
    "# print_market_summary(all_tickers)\n",
    "# all_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f37df0",
   "metadata": {},
   "source": [
    "### Download tickers from S&P500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "\n",
    "# sp500['Symbol'] = sp500['Symbol'].str.replace('.', '-')\n",
    "\n",
    "# symbols_list = sp500['Symbol'].unique().tolist()\n",
    "\n",
    "# # see https://fingpt.bot/ for stock prediction\n",
    "# additional_symbols = ['ASML', 'VICI', 'WST','WYNN','ZBH','ABNB','AMZN','FTNT','GEHC','MRVL']\n",
    "# # je bent heiiiiiir\n",
    "# COMBINED_LIST = symbols_list + additional_symbols\n",
    "# # remove double entries\n",
    "# symbols_list = list(set(COMBINED_LIST))\n",
    "\n",
    "# print(symbols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954b9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18a80b5a",
   "metadata": {},
   "source": [
    "### Download tickers from S&P500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63106e",
   "metadata": {},
   "source": [
    "### Download tickers from Nasdaq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a04cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nasdaq_tickers(no_tickers=3000):\n",
    "#     url = 'https://api.nasdaq.com/api/screener/stocks?tableonly=true&limit=25&offset=0&download=true'\n",
    "\n",
    "#     headers = {\n",
    "#             'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "#             }\n",
    "\n",
    "#     resp = requests.get(url,headers=headers)\n",
    "#     json_data = resp.json()\n",
    "#     df = pd.DataFrame(json_data['data']['rows'],columns=json_data['data']['headers'])\n",
    "\n",
    "#     ## convert columns to the correct data types\n",
    "#     # df['lastsale'] = df['lastsale'].astype(float)\n",
    "\n",
    "#     ## convert lastsale remove $ and convert to float\n",
    "#     df['lastsale'] = df['lastsale'].str.replace('$','').astype(float)\n",
    "#     ## convert netchange into float\n",
    "#     df['netchange'] = df['netchange'].str.replace('$','').astype(float)\n",
    "#     # convert pctchange remove % and convert to float\n",
    "#     df['pctchange'] = pd.to_numeric(df['pctchange'].str.replace('%', ''), errors='coerce')\n",
    "#     ## convert marketCap to string to float\n",
    "#     df['marketCap'] = pd.to_numeric(df['marketCap'], errors='coerce')\n",
    "#     ## convert volume to int\n",
    "#     df['volume'] = df['volume'].str.replace(',','').astype(int)\n",
    "\n",
    "#     df.to_csv('nasdaq.csv',index=False)\n",
    "\n",
    "#     ### Select top 50 stocks with the highigest trade volume.\n",
    "\n",
    "#     # Filter df and select the 100 rows with the highest volume\n",
    "#     # df = df.sort_values('volume',ascending=False).head(no_tickers)\n",
    "    \n",
    "#    # hier moet je beter gaan filteren en de meest kansrijke aandelen selecteren\n",
    "\n",
    "#     # tickers contains a list of df symbol and name \n",
    "#     tickers = list(df[['symbol','name']].itertuples(index=False, name=None))\n",
    "#     # take tickers and copy the column symbol to a list\n",
    "#     tickers = list(df['symbol'])\n",
    "    \n",
    "\n",
    "#     return tickers, df\n",
    "\n",
    "# tickers, df = get_nasdaq_tickers(100)\n",
    "# print(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeddf97",
   "metadata": {},
   "source": [
    "## 1. Download/Load  stocks prices data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_date = '2023-09-27'\n",
    "# # end_date = '2024-05-1'\n",
    "\n",
    "# start_date = pd.to_datetime(end_date)-pd.DateOffset(365*8)\n",
    "\n",
    "# Calculate the date 8 years ago from today 8*365\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=8*365)  # Approximation, does not account for leap years\n",
    "\n",
    "# Format the dates in a way that yfinance expects\n",
    "start_date = start_date.strftime('%Y-%m-%d')\n",
    "end_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "df = yf.download(tickers=symbols_list,\n",
    "                 start=start_date,\n",
    "                 end=end_date).stack()\n",
    "\n",
    "df.to_csv('downloaded_stocks.csv')\n",
    "\n",
    "df.index.names = ['date', 'ticker']\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c34ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55746c31",
   "metadata": {},
   "source": [
    "## 2. Calculate features and technical indicators for each stock.\n",
    "\n",
    "* Garman-Klass Volatility\n",
    "* RSI\n",
    "* Bollinger Bands\n",
    "* ATR\n",
    "* MACD\n",
    "* Dollar Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94feae",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{Garman-Klass Volatility} = \\frac{(\\ln(\\text{High}) - \\ln(\\text{Low}))^2}{2} - (2\\ln(2) - 1)(\\ln(\\text{Adj Close}) - \\ln(\\text{Open}))^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b0516",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481fd2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tickers with fewer than 20 rows of data so that we can calculate the 20-day moving average\n",
    "df_filtered = df.groupby('ticker').filter(lambda x: len(x) >= 20)\n",
    "\n",
    "# Ensure the DataFrame `df` retains the same format\n",
    "df = df_filtered\n",
    "\n",
    "df['garman_klass_vol'] = ((np.log(df['high'])-np.log(df['low']))**2)/2-(2*np.log(2)-1)*((np.log(df['adj close'])-np.log(df['open']))**2)\n",
    "\n",
    "df['rsi'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.rsi(close=x, length=20))\n",
    "\n",
    "df['bb_low'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,0])\n",
    "                                                        \n",
    "df['bb_mid'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,1])\n",
    "                                                        \n",
    "df['bb_high'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,2])\n",
    "\n",
    "def compute_atr(stock_data):\n",
    "    atr = pandas_ta.atr(high=stock_data['high'],\n",
    "                        low=stock_data['low'],\n",
    "                        close=stock_data['close'],\n",
    "                        length=14)\n",
    "    return atr.sub(atr.mean()).div(atr.std())\n",
    "\n",
    "df['atr'] = df.groupby(level=1, group_keys=False).apply(compute_atr)\n",
    "\n",
    "def compute_macd(close):\n",
    "   \n",
    "    # # if len(close) < 25:\n",
    "    # #     print(\"Not enough data to compute MACD\")\n",
    "    # #     return None\n",
    "    # macd = pandas_ta.macd(close=close, length=20)\n",
    "    # print ('Macd - ',macd.info())\n",
    "    # macd = macd.iloc[:,0]\n",
    "    # return macd.sub(macd.mean()).div(macd.std())\n",
    "\n",
    "   \n",
    "    if close.size < 25:  # Ensure there are enough data points for MACD calculation\n",
    "        return pd.Series([None] * len(close), index=close.index)\n",
    "\n",
    "    try:\n",
    "        # Assuming 'close' is a pandas Series of closing prices\n",
    "        macd = pandas_ta.macd(close=close, length=20)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Handle the error or set macd to None or an empty DataFrame\n",
    "        return pd.Series([None] * len(close), index=close.index)\n",
    "\n",
    "    if macd is None or macd.empty:\n",
    "        print(\"Debug: MACD calculation returned None or empty for data:\", close)\n",
    "        return pd.Series([None] * len(close), index=close.index)\n",
    "    macd_series = macd.iloc[:, 0]  # Assuming the first column is the MACD line\n",
    "    return macd_series.sub(macd_series.mean()).div(macd_series.std())\n",
    "\n",
    "\n",
    "df['macd'] = df.groupby(level=1, group_keys=False)['adj close'].apply(compute_macd)\n",
    "\n",
    "df['dollar_volume'] = (df['adj close']*df['volume'])/1e6\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7ef78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c735696b",
   "metadata": {},
   "source": [
    "## 3. Aggregate to monthly level and filter top 150 most liquid stocks for each month.\n",
    "\n",
    "* To reduce training time and experiment with features and strategies, we convert the business-daily data to month-end frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec67c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cols = [c for c in df.columns.unique(0) if c not in ['dollar_volume', 'volume', 'open',\n",
    "                                                          'high', 'low', 'close']]\n",
    "\n",
    "\n",
    "data = (pd.concat([df.unstack('ticker')['dollar_volume'].resample('M').mean().stack('ticker').to_frame('dollar_volume'),\n",
    "                   df.unstack()[last_cols].resample('M').last().stack('ticker')],\n",
    "                  axis=1)).dropna()\n",
    "\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632ffc7",
   "metadata": {},
   "source": [
    "* Calculate 5-year rolling average of dollar volume for each stocks before filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5208030",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dollar_volume'] = (data.loc[:, 'dollar_volume'].unstack('ticker').rolling(5*12, min_periods=12).mean().stack())\n",
    "\n",
    "data['dollar_vol_rank'] = (data.groupby('date')['dollar_volume'].rank(ascending=False))\n",
    "\n",
    "data = data[data['dollar_vol_rank']<150].drop(['dollar_volume', 'dollar_vol_rank'], axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e13a3b",
   "metadata": {},
   "source": [
    "## 4. Calculate Monthly Returns for different time horizons as features.\n",
    "\n",
    "* To capture time series dynamics that reflect, for example, momentum patterns, we compute historical returns using the method .pct_change(lag), that is, returns over various monthly periods as identified by lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(df):\n",
    "\n",
    "    outlier_cutoff = 0.005\n",
    "\n",
    "    lags = [1, 2, 3, 6, 9, 12]\n",
    "\n",
    "    for lag in lags:\n",
    "\n",
    "        df[f'return_{lag}m'] = (df['adj close']\n",
    "                              .pct_change(lag)\n",
    "                              .pipe(lambda x: x.clip(lower=x.quantile(outlier_cutoff),\n",
    "                                                     upper=x.quantile(1-outlier_cutoff)))\n",
    "                              .add(1)\n",
    "                              .pow(1/lag)\n",
    "                              .sub(1))\n",
    "    return df\n",
    "    \n",
    "    \n",
    "data = data.groupby(level=1, group_keys=False).apply(calculate_returns).dropna()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df434b55",
   "metadata": {},
   "source": [
    "## 5. Download Fama-French Factors and Calculate Rolling Factor Betas.\n",
    "\n",
    "* We will introduce the Fama—French data to estimate the exposure of assets to common risk factors using linear regression.\n",
    "\n",
    "* The five Fama—French factors, namely market risk, size, value, operating profitability, and investment have been shown empirically to explain asset returns and are commonly used to assess the risk/return profile of portfolios. Hence, it is natural to include past factor exposures as financial features in models.\n",
    "\n",
    "* We can access the historical factor returns using the pandas-datareader and estimate historical exposures using the RollingOLS rolling linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = web.DataReader('F-F_Research_Data_5_Factors_2x3',\n",
    "                               'famafrench',\n",
    "                               start='2010')[0].drop('RF', axis=1)\n",
    "\n",
    "factor_data.index = factor_data.index.to_timestamp()\n",
    "\n",
    "factor_data = factor_data.resample('M').last().div(100)\n",
    "\n",
    "factor_data.index.name = 'date'\n",
    "\n",
    "factor_data = factor_data.join(data['return_1m']).sort_index()\n",
    "\n",
    "factor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840bf3b",
   "metadata": {},
   "source": [
    "* Filter out stocks with less than 10 months of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = factor_data.groupby(level=1).size()\n",
    "\n",
    "valid_stocks = observations[observations >= 10]\n",
    "\n",
    "factor_data = factor_data[factor_data.index.get_level_values('ticker').isin(valid_stocks.index)]\n",
    "\n",
    "factor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0107433",
   "metadata": {},
   "source": [
    "* Calculate Rolling Factor Betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = (factor_data.groupby(level=1,\n",
    "                            group_keys=False)\n",
    "         .apply(lambda x: RollingOLS(endog=x['return_1m'], \n",
    "                                     exog=sm.add_constant(x.drop('return_1m', axis=1)),\n",
    "                                     window=min(24, x.shape[0]),\n",
    "                                     min_nobs=len(x.columns)+1)\n",
    "         .fit(params_only=True)\n",
    "         .params\n",
    "         .drop('const', axis=1)))\n",
    "\n",
    "betas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491e61a",
   "metadata": {},
   "source": [
    "* Join the rolling factors data to the main features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e169594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "data = (data.join(betas.groupby('ticker').shift()))\n",
    "\n",
    "data.loc[:, factors] = data.groupby('ticker', group_keys=False)[factors].apply(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "data = data.drop('adj close', axis=1)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eeb7f6",
   "metadata": {},
   "source": [
    "### At this point we have to decide on what ML model and approach to use for predictions etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785bbfb3",
   "metadata": {},
   "source": [
    "## 6. For each month fit a K-Means Clustering Algorithm to group similar assets based on their features.\n",
    "\n",
    "### K-Means Clustering\n",
    "* You may want to initialize predefined centroids for each cluster based on your research.\n",
    "\n",
    "* For visualization purpose of this tutorial we will initially rely on the ‘k-means++’ initialization.\n",
    "\n",
    "* Then we will pre-define our centroids for each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c74065",
   "metadata": {},
   "source": [
    "### Apply pre-defined centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a94dc1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_rsi_values = [30, 45, 55, 70]\n",
    "\n",
    "initial_centroids = np.zeros((len(target_rsi_values), 18))\n",
    "\n",
    "initial_centroids[:, 6] = target_rsi_values\n",
    "\n",
    "initial_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323ab80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "if 'cluster' in data.columns:\n",
    "    data = data.drop('cluster', axis=1)\n",
    "\n",
    "\n",
    "def get_clusters(df):\n",
    "    df['cluster'] = KMeans(n_clusters=4,\n",
    "                           random_state=0,\n",
    "                           init=initial_centroids).fit(df).labels_\n",
    "    return df\n",
    "\n",
    "data = data.dropna().groupby('date', group_keys=False).apply(get_clusters)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(data):\n",
    "\n",
    "    cluster_0 = data[data['cluster']==0]\n",
    "    cluster_1 = data[data['cluster']==1]\n",
    "    cluster_2 = data[data['cluster']==2]\n",
    "    cluster_3 = data[data['cluster']==3]\n",
    "\n",
    "    plt.scatter(cluster_0.iloc[:,0] , cluster_0.iloc[:,6] , color = 'red', label='cluster 0')\n",
    "    plt.scatter(cluster_1.iloc[:,0] , cluster_1.iloc[:,6] , color = 'green', label='cluster 1')\n",
    "    plt.scatter(cluster_2.iloc[:,0] , cluster_2.iloc[:,6] , color = 'blue', label='cluster 2')\n",
    "    plt.scatter(cluster_3.iloc[:,0] , cluster_3.iloc[:,6] , color = 'black', label='cluster 3')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984bd52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "for i in data.index.get_level_values('date').unique().tolist():\n",
    "    \n",
    "    g = data.xs(i, level=0)\n",
    "    \n",
    "    plt.title(f'Date {i}')\n",
    "    \n",
    "    plot_clusters(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28c49e",
   "metadata": {},
   "source": [
    "## 7. For each month select assets based on the cluster and form a portfolio based on Efficient Frontier max sharpe ratio optimization\n",
    "\n",
    "* First we will filter only stocks corresponding to the cluster we choose based on our hypothesis.\n",
    "\n",
    "* Momentum is persistent and my idea would be that stocks clustered around RSI 70 centroid should continue to outperform in the following month - thus I would select stocks corresponding to cluster 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = data[data['cluster']==3].copy()\n",
    "\n",
    "filtered_df = filtered_df.reset_index(level=1)\n",
    "\n",
    "filtered_df.index = filtered_df.index+pd.DateOffset(1)\n",
    "\n",
    "filtered_df = filtered_df.reset_index().set_index(['date', 'ticker'])\n",
    "\n",
    "dates = filtered_df.index.get_level_values('date').unique().tolist()\n",
    "\n",
    "fixed_dates = {}\n",
    "\n",
    "for d in dates:\n",
    "    \n",
    "    fixed_dates[d.strftime('%Y-%m-%d')] = filtered_df.xs(d, level=0).index.tolist()\n",
    "    \n",
    "fixed_dates\n",
    "\n",
    "# convert fixed_dates to a pandas DataFrame\n",
    "# Identify the last key in the dictionary\n",
    "last_key = list(fixed_dates.keys())[-1]\n",
    "\n",
    "# Extract the data associated with the last key\n",
    "last_column_data = fixed_dates[last_key]\n",
    "\n",
    "# Create a pandas DataFrame from the extracted data\n",
    "watchlist = pd.DataFrame({last_key: last_column_data})\n",
    "watchlist.columns.values[0] = 'Symbol'\n",
    "watchlist['Exchange'] = 'NASDAQ'\n",
    "watchlist['Type'] = 'Stock'\n",
    "\n",
    "watchlist.to_csv('watchlist.csv', index=False)\n",
    "\n",
    "\n",
    "# Display the DataFrame\n",
    "print(watchlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a7fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract the last entry\n",
    "# Get the last key based on maximum date\n",
    "last_date = max(fixed_dates.keys())\n",
    "last_date_stocks = fixed_dates[last_date]\n",
    "\n",
    "# Step 2: Copy the values to a list (already in list form, directly use it)\n",
    "stocks_to_save = last_date_stocks\n",
    "\n",
    "# Step 3: Save the list to a CSV file\n",
    "csv_filename = 'stocks_list.csv'\n",
    "with open(csv_filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Stocks'])  # Writing a header, optional\n",
    "    for stock in stocks_to_save:\n",
    "        writer.writerow([stock])  # Each stock in its own row\n",
    "\n",
    "print(f\"Data from the last date {last_date} has been saved to {csv_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f9941",
   "metadata": {},
   "source": [
    "### Define portfolio optimization function\n",
    "\n",
    "* We will define a function which optimizes portfolio weights using PyPortfolioOpt package and EfficientFrontier optimizer to maximize the sharpe ratio.\n",
    "\n",
    "* To optimize the weights of a given portfolio we would need to supply last 1 year prices to the function.\n",
    "\n",
    "* Apply signle stock weight bounds constraint for diversification (minimum half of equaly weight and maximum 10% of portfolio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2888d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "\n",
    "def optimize_weights(prices, lower_bound=0):\n",
    "    \n",
    "    returns = expected_returns.mean_historical_return(prices=prices,\n",
    "                                                      frequency=252)\n",
    "    \n",
    "    cov = risk_models.sample_cov(prices=prices,\n",
    "                                 frequency=252)\n",
    "    \n",
    "    ef = EfficientFrontier(expected_returns=returns,\n",
    "                           cov_matrix=cov,\n",
    "                           weight_bounds=(lower_bound, .1),\n",
    "                           solver='SCS')\n",
    "    \n",
    "    weights = ef.max_sharpe()\n",
    "    \n",
    "    return ef.clean_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd59c17",
   "metadata": {},
   "source": [
    "* Download Fresh Daily Prices Data only for short listed stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d737d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = data.index.get_level_values('ticker').unique().tolist()\n",
    "\n",
    "new_df = yf.download(tickers=stocks,\n",
    "                     start=data.index.get_level_values('date').unique()[0]-pd.DateOffset(months=12),\n",
    "                     end=data.index.get_level_values('date').unique()[-1])\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b906e",
   "metadata": {},
   "source": [
    "* Calculate daily returns for each stock which could land up in our portfolio.\n",
    "\n",
    "* Then loop over each month start, select the stocks for the month and calculate their weights for the next month.\n",
    "\n",
    "* If the maximum sharpe ratio optimization fails for a given month, apply equally-weighted weights.\n",
    "\n",
    "* Calculated each day portfolio return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_dataframe = np.log(new_df['Adj Close']).diff()\n",
    "\n",
    "portfolio_df = pd.DataFrame()\n",
    "\n",
    "for start_date in fixed_dates.keys():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        end_date = (pd.to_datetime(start_date)+pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')\n",
    "\n",
    "        cols = fixed_dates[start_date]\n",
    "\n",
    "        optimization_start_date = (pd.to_datetime(start_date)-pd.DateOffset(months=12)).strftime('%Y-%m-%d')\n",
    "\n",
    "        optimization_end_date = (pd.to_datetime(start_date)-pd.DateOffset(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        optimization_df = new_df[optimization_start_date:optimization_end_date]['Adj Close'][cols]\n",
    "        \n",
    "        success = False\n",
    "        try:\n",
    "            weights = optimize_weights(prices=optimization_df,\n",
    "                                   lower_bound=round(1/(len(optimization_df.columns)*2),3))\n",
    "\n",
    "            weights = pd.DataFrame(weights, index=pd.Series(0))\n",
    "            \n",
    "            success = True\n",
    "        except:\n",
    "            print(f'Max Sharpe Optimization failed for {start_date}, Continuing with Equal-Weights')\n",
    "        \n",
    "        if success==False:\n",
    "            weights = pd.DataFrame([1/len(optimization_df.columns) for i in range(len(optimization_df.columns))],\n",
    "                                     index=optimization_df.columns.tolist(),\n",
    "                                     columns=pd.Series(0)).T\n",
    "        \n",
    "        temp_df = returns_dataframe[start_date:end_date]\n",
    "\n",
    "        temp_df = temp_df.stack().to_frame('return').reset_index(level=0)\\\n",
    "                   .merge(weights.stack().to_frame('weight').reset_index(level=0, drop=True),\n",
    "                          left_index=True,\n",
    "                          right_index=True)\\\n",
    "                   .reset_index().set_index(['Date', 'index']).unstack().stack()\n",
    "\n",
    "        temp_df.index.names = ['date', 'ticker']\n",
    "\n",
    "        temp_df['weighted_return'] = temp_df['return']*temp_df['weight']\n",
    "\n",
    "        temp_df = temp_df.groupby(level=0)['weighted_return'].sum().to_frame('Strategy Return')\n",
    "\n",
    "        portfolio_df = pd.concat([portfolio_df, temp_df], axis=0)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "portfolio_df = portfolio_df.drop_duplicates()\n",
    "\n",
    "portfolio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcced0d1",
   "metadata": {},
   "source": [
    "## 8. Visualize Portfolio returns and compare to SP500 returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy = yf.download(tickers='SPY',\n",
    "                  start='2015-01-01',\n",
    "                  end=dt.date.today())\n",
    "\n",
    "spy_ret = np.log(spy[['Adj Close']]).diff().dropna().rename({'Adj Close':'SPY Buy&Hold'}, axis=1)\n",
    "\n",
    "portfolio_df = portfolio_df.merge(spy_ret,\n",
    "                                  left_index=True,\n",
    "                                  right_index=True)\n",
    "\n",
    "portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "portfolio_cumulative_return = np.exp(np.log1p(portfolio_df).cumsum())-1\n",
    "\n",
    "portfolio_cumulative_return[:'2023-09-29'].plot(figsize=(16,6))\n",
    "\n",
    "plt.title('Unsupervised Learning Trading Strategy Returns Over Time')\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "plt.ylabel('Return')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a75c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a8a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f18007",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Investing Strategy\n",
    "\n",
    "## 1. Load Twitter Sentiment Data\n",
    "\n",
    "* Load the twitter sentiment dataset, set the index, calculat engagement ratio and filter out stocks with no significant twitter activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import datetime as dt\n",
    "# import yfinance as yf\n",
    "# import os\n",
    "# plt.style.use('ggplot')\n",
    "\n",
    "# data_folder = 'C:/Users/user/Desktop/Python Scripts'\n",
    "\n",
    "# sentiment_df = pd.read_csv(os.path.join(data_folder, 'sentiment_data.csv'))\n",
    "\n",
    "# sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "\n",
    "# sentiment_df = sentiment_df.set_index(['date', 'symbol'])\n",
    "\n",
    "# sentiment_df['engagement_ratio'] = sentiment_df['twitterComments']/sentiment_df['twitterLikes']\n",
    "\n",
    "# sentiment_df = sentiment_df[(sentiment_df['twitterLikes']>20)&(sentiment_df['twitterComments']>10)]\n",
    "\n",
    "# sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde71915",
   "metadata": {},
   "source": [
    "## 2. Aggregate Monthly and calculate average sentiment for the month\n",
    "\n",
    "* Aggregate on a monthly level and calculate average monthly metric, for the one we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1158a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggragated_df = (sentiment_df.reset_index('symbol').groupby([pd.Grouper(freq='M'), 'symbol'])\n",
    "#                     [['engagement_ratio']].mean())\n",
    "\n",
    "# aggragated_df['rank'] = (aggragated_df.groupby(level=0)['engagement_ratio']\n",
    "#                          .transform(lambda x: x.rank(ascending=False)))\n",
    "\n",
    "# aggragated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9f188",
   "metadata": {},
   "source": [
    "## 3. Select Top 5 Stocks based on their cross-sectional ranking for each month\n",
    "\n",
    "* Select top 5 stocks by rank for each month and fix the date to start at beginning of next month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = aggragated_df[aggragated_df['rank']<6].copy()\n",
    "\n",
    "# filtered_df = filtered_df.reset_index(level=1)\n",
    "\n",
    "# filtered_df.index = filtered_df.index+pd.DateOffset(1)\n",
    "\n",
    "# filtered_df = filtered_df.reset_index().set_index(['date', 'symbol'])\n",
    "\n",
    "# filtered_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecadd7",
   "metadata": {},
   "source": [
    "## 4. Extract the stocks to form portfolios with at the start of each new month\n",
    "\n",
    "* Create a dictionary containing start of month and corresponded selected stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc646c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates = filtered_df.index.get_level_values('date').unique().tolist()\n",
    "\n",
    "# fixed_dates = {}\n",
    "\n",
    "# for d in dates:\n",
    "    \n",
    "#     fixed_dates[d.strftime('%Y-%m-%d')] = filtered_df.xs(d, level=0).index.tolist()\n",
    "    \n",
    "# fixed_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9750aea",
   "metadata": {},
   "source": [
    "## 5. Download fresh stock prices for only selected/shortlisted stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6503bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks_list = sentiment_df.index.get_level_values('symbol').unique().tolist()\n",
    "\n",
    "# prices_df = yf.download(tickers=stocks_list,\n",
    "#                         start='2021-01-01',\n",
    "#                         end='2023-03-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64da79b",
   "metadata": {},
   "source": [
    "## 6. Calculate Portfolio Returns with monthly rebalancing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94971d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns_df = np.log(prices_df['Adj Close']).diff().dropna()\n",
    "\n",
    "# portfolio_df = pd.DataFrame()\n",
    "\n",
    "# for start_date in fixed_dates.keys():\n",
    "    \n",
    "#     end_date = (pd.to_datetime(start_date)+pd.offsets.MonthEnd()).strftime('%Y-%m-%d')\n",
    "    \n",
    "#     cols = fixed_dates[start_date]\n",
    "    \n",
    "#     temp_df = returns_df[start_date:end_date][cols].mean(axis=1).to_frame('portfolio_return')\n",
    "    \n",
    "#     portfolio_df = pd.concat([portfolio_df, temp_df], axis=0)\n",
    "    \n",
    "# portfolio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3f0d7",
   "metadata": {},
   "source": [
    "## 7. Download NASDAQ/QQQ prices and calculate returns to compare to our strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qqq_df = yf.download(tickers='QQQ',\n",
    "#                      start='2021-01-01',\n",
    "#                      end='2023-03-01')\n",
    "\n",
    "# qqq_ret = np.log(qqq_df['Adj Close']).diff().to_frame('nasdaq_return')\n",
    "\n",
    "# portfolio_df = portfolio_df.merge(qqq_ret,\n",
    "#                                   left_index=True,\n",
    "#                                   right_index=True)\n",
    "\n",
    "# portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf728f75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# portfolios_cumulative_return = np.exp(np.log1p(portfolio_df).cumsum()).sub(1)\n",
    "\n",
    "# portfolios_cumulative_return.plot(figsize=(16,6))\n",
    "\n",
    "# plt.title('Twitter Engagement Ratio Strategy Return Over Time')\n",
    "\n",
    "# plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "# plt.ylabel('Return')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f25123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51f43be9",
   "metadata": {},
   "source": [
    "# Intraday Strategy Using GARCH Model\n",
    "\n",
    "\n",
    "* Using simulated daily data and intraday 5-min data.\n",
    "* Load Daily and 5-minute data.\n",
    "* Define function to fit GARCH model on the daily data and predict 1-day ahead volatility in a rolling window.\n",
    "* Calculate prediction premium and form a daily signal from it.\n",
    "* Merge with intraday data and calculate intraday indicators to form the intraday signal.\n",
    "* Generate the position entry and hold until the end of the day.\n",
    "* Calculate final strategy returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be3499",
   "metadata": {},
   "source": [
    "## 1. Load Simulated Daily and Simulated 5-minute data.\n",
    "\n",
    "* We are loading both datasets, set the indexes and calculate daily log returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from arch import arch_model\n",
    "# import pandas_ta\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# data_folder = 'C:/Users/user/Desktop/Python Scripts'\n",
    "\n",
    "# daily_df = pd.read_csv(os.path.join(data_folder, 'simulated_daily_data.csv'))\n",
    "\n",
    "# daily_df = daily_df.drop('Unnamed: 7', axis=1)\n",
    "\n",
    "# daily_df['Date'] = pd.to_datetime(daily_df['Date'])\n",
    "\n",
    "# daily_df = daily_df.set_index('Date')\n",
    "\n",
    "\n",
    "# intraday_5min_df = pd.read_csv(os.path.join(data_folder, 'simulated_5min_data.csv'))\n",
    "\n",
    "# intraday_5min_df = intraday_5min_df.drop('Unnamed: 6', axis=1)\n",
    "\n",
    "# intraday_5min_df['datetime'] = pd.to_datetime(intraday_5min_df['datetime'])\n",
    "\n",
    "# intraday_5min_df = intraday_5min_df.set_index('datetime')\n",
    "\n",
    "# intraday_5min_df['date'] = pd.to_datetime(intraday_5min_df.index.date)\n",
    "\n",
    "# intraday_5min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b9cc4",
   "metadata": {},
   "source": [
    "## 2. Define function to fit GARCH model and predict 1-day ahead volatility in a rolling window.\n",
    "\n",
    "* We are first calculating the 6-month rolling variance and then we are creating a function in a 6-month rolling window to fit a garch model and predict the next day variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed18e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# daily_df['log_ret'] = np.log(daily_df['Adj Close']).diff()\n",
    "\n",
    "# daily_df['variance'] = daily_df['log_ret'].rolling(180).var()\n",
    "\n",
    "# daily_df = daily_df['2020':]\n",
    "\n",
    "# def predict_volatility(x):\n",
    "    \n",
    "#     best_model = arch_model(y=x,\n",
    "#                             p=1,\n",
    "#                             q=3).fit(update_freq=5,\n",
    "#                                      disp='off')\n",
    "    \n",
    "#     variance_forecast = best_model.forecast(horizon=1).variance.iloc[-1,0]\n",
    "\n",
    "#     print(x.index[-1])\n",
    "    \n",
    "#     return variance_forecast\n",
    "\n",
    "# daily_df['predictions'] = daily_df['log_ret'].rolling(180).apply(lambda x: predict_volatility(x))\n",
    "\n",
    "# daily_df = daily_df.dropna()\n",
    "\n",
    "# daily_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d692730",
   "metadata": {},
   "source": [
    "## 3. Calculate prediction premium and form a daily signal from it.\n",
    "\n",
    "* We are calculating the prediction premium. And calculate its 6-month rolling standard deviation.\n",
    "\n",
    "* From this we are creating our daily signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb535a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_df['prediction_premium'] = (daily_df['predictions']-daily_df['variance'])/daily_df['variance']\n",
    "\n",
    "# daily_df['premium_std'] = daily_df['prediction_premium'].rolling(180).std()\n",
    "\n",
    "# daily_df['signal_daily'] = daily_df.apply(lambda x: 1 if (x['prediction_premium']>x['premium_std'])\n",
    "#                                          else (-1 if (x['prediction_premium']<x['premium_std']*-1) else np.nan),\n",
    "#                                          axis=1)\n",
    "\n",
    "# daily_df['signal_daily'] = daily_df['signal_daily'].shift()\n",
    "\n",
    "# daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd44075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('ggplot')\n",
    "\n",
    "# daily_df['signal_daily'].plot(kind='hist')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467e501",
   "metadata": {},
   "source": [
    "## 4. Merge with intraday data and calculate intraday indicators to form the intraday signal.\n",
    "\n",
    "* Calculate all intraday indicators and intraday signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d99a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = intraday_5min_df.reset_index()\\\n",
    "#                             .merge(daily_df[['signal_daily']].reset_index(),\n",
    "#                                    left_on='date',\n",
    "#                                    right_on='Date')\\\n",
    "#                             .drop(['date','Date'], axis=1)\\\n",
    "#                             .set_index('datetime')\n",
    "\n",
    "# final_df['rsi'] = pandas_ta.rsi(close=final_df['close'],\n",
    "#                                 length=20)\n",
    "\n",
    "# final_df['lband'] = pandas_ta.bbands(close=final_df['close'],\n",
    "#                                      length=20).iloc[:,0]\n",
    "\n",
    "# final_df['uband'] = pandas_ta.bbands(close=final_df['close'],\n",
    "#                                      length=20).iloc[:,2]\n",
    "\n",
    "# final_df['signal_intraday'] = final_df.apply(lambda x: 1 if (x['rsi']>70)&\n",
    "#                                                             (x['close']>x['uband'])\n",
    "#                                              else (-1 if (x['rsi']<30)&\n",
    "#                                                          (x['close']<x['lband']) else np.nan),\n",
    "#                                              axis=1)\n",
    "\n",
    "# final_df['return'] = np.log(final_df['close']).diff()\n",
    "\n",
    "# final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae154db7",
   "metadata": {},
   "source": [
    "## 5. Generate the position entry and hold until the end of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df['return_sign'] = final_df.apply(lambda x: -1 if (x['signal_daily']==1)&(x['signal_intraday']==1)\n",
    "#                                         else (1 if (x['signal_daily']==-1)&(x['signal_intraday']==-1) else np.nan),\n",
    "#                                         axis=1)\n",
    "\n",
    "# final_df['return_sign'] = final_df.groupby(pd.Grouper(freq='D'))['return_sign']\\\n",
    "#                                   .transform(lambda x: x.ffill())\n",
    "\n",
    "# final_df['forward_return'] = final_df['return'].shift(-1)\n",
    "\n",
    "# final_df['strategy_return'] = final_df['forward_return']*final_df['return_sign']\n",
    "\n",
    "# daily_return_df = final_df.groupby(pd.Grouper(freq='D'))['strategy_return'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd2e02",
   "metadata": {},
   "source": [
    "## 6. Calculate final strategy returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93caf5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.ticker as mtick\n",
    "\n",
    "# strategy_cumulative_return = np.exp(np.log1p(daily_return_df).cumsum()).sub(1)\n",
    "\n",
    "# strategy_cumulative_return.plot(figsize=(16,6))\n",
    "\n",
    "# plt.title('Intraday Strategy Returns')\n",
    "\n",
    "# plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "# plt.ylabel('Return')\n",
    "\n",
    "# plt.show()\n",
    "                                                                            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolioGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
